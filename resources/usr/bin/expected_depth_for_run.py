#!/usr/bin/python2
# 
# 
# 
# 
# Kim Brugger (23 Jan 2018), contact: kim@brugger.dk
# Pavlos Antoniou (14 Aug 2018), contact: pavlos.antoniou@addenbrookes.nhs.uk


import sys
import os
import re
import gzip
import argparse
from math import sqrt
import pysam
import ccbg.toolbox as toolbox
import pprint
pp = pprint.PrettyPrinter(indent=4)


# Expected depths in ouput will be normalised represent a sample with 100M reads
NORMAL_READS = 100000000


def readin_flagstats(filenames):

    res = {}
    for filename in filenames:

        sample_name = toolbox.get_sample_name(filename)

        res[ sample_name ] = {}

        fh = open (filename, 'Ur')
        for line in fh.readlines():
            line = line.strip("\n")

            if re.search('total', line):
                res[sample_name]['total_reads'] = re.sub(r'^(\d+) .*reads.*', r'\1', line)

            elif re.search(r'duplicates', line):
                res[sample_name]['dups_reads'] = re.sub(r'^(\d+) .*duplicates', r'\1', line)

            elif re.search(r'mapped \(', line):
                res[sample_name]['mapped_reads']= re.sub(r'^(\d+) .*mapped.*', r'\1', line)

            elif re.search(r'properly paired', line):
                res[sample_name]['properly paired']= re.sub(r'^(\d+) .*properly paired .*', r'\1', line)

            elif re.search(r'singletons', line):
                res[sample_name]['singletons']= re.sub(r'^(\d+) .*singletons .*', r'\1', line)

        res[sample_name]['usable_reads'] = int(res[sample_name]['mapped_reads']) - int(res[sample_name]['dups_reads'])

        fh.close()

    return res


def calculate_exp_depth(flagstats, filenames, outfile, force=False ):
    
    fhs = {}

    for filename in filenames:

        sample_name = toolbox.get_sample_name(filename)
        fhs[sample_name] = gzip.open(filename)

    fh = None
    if (outfile is not None):
        fh = open(outfile, 'w')
        fh.write("# File generated by expected_depth_for_run.py\n")
        fh.write("# Args: {}\n".format(", ".join(filenames)))
    else:
        print("# File generated by expected_depth_for_run.py")
        print("# Args: {}".format(", ".join(filenames)))


    while(True):
        depths = []
        chrom = None
        start = None
        end   = None
        comment_line = False
        for sample_name in fhs:
            depth = fhs[sample_name].readline()

            # we dont have flagstats infor for this sample, so skip it
            if sample_name not in flagstats:
                continue

            if ('#' in depth):
                comment_line = True                
                continue

            depth = depth.rstrip("\n")
            fields = depth.split("\t")
            if (len(fields) < 2):
                print "Error line: '{}'".format(depth)
                continue

            if (chrom is None):
                chrom = fields[0]
                start = fields[1]
                end   = fields[2]

            if ( chrom != fields[0] and start != fields[1] and end != fields[2]):
                region = "{chrom}:{start}-{end}"
                print("Files out of sync region {}: does not match region {}".format(region.format(chrom, start, end), 
                                                                                      region.format(fields[0], fields[1], fields[2])))
                exit(-10)

            mean_depth = float(fields[4])
            mean_depth = mean_depth * NORMAL_READS/flagstats[sample_name]['usable_reads']

            depths.append(mean_depth)

        if (comment_line):
            continue

        if (len(depths) == 0):
            print "Breaking on missing depths"
            break

        if (fh is not None):
            fh.write("{}\t{}\t{}\t{}\t{}\n".format(chrom, start, end, sum(depths)/len(depths), standard_deviation(depths)))
        else:
            print "{}\t{}\t{}\t{}\t{}".format(chrom, start, end, sum(depths)/len(depths), standard_deviation(depths))

    if (outfile is not None):
        fh.close()
        pysam.tabix_index(outfile, seq_col=0, start_col=1, end_col=2, force=force)



def standard_deviation(values):
    """ Calculate the standard deviantion from a set of values

    Args:
      Values (list of int/float)

    Returns:
      Standard deviation (float)

    """

    if len(values) = 1:
        return 0
        
    sqr_sum = 0.0

    for value in values:
        sqr_sum += float(value)**2

    value_sum = sum(values)
    std_dev = sqrt((len(values)*sqr_sum-value_sum*value_sum)/(len(values)*(len(values)-1)))

    return std_dev


if __name__ == "__main__":
    
    parser = argparse.ArgumentParser()

    parser = argparse.ArgumentParser(description='Calculates the expected depth for regions ')
    
    parser.add_argument('--depths',    nargs='+')
    parser.add_argument('--flagstats', nargs='+')
    parser.add_argument('-o','--outfile', help='File to write to, if done it will be compressed and indexed as well')
    parser.add_argument('-F', '--force-overwrite', action="store_true", default=False,  help="overwrite old file if present")


    args = parser.parse_args()

    if args.outfile is not None:
      if os.path.isfile( "{}.gz".format(args.outfile)) and not args.force_overwrite:
        print("Error file already exists, to overwrite use the -f flag")
        exit(-10)

    if (args.flagstats is None or args.depths is None):
        parser.parse_args(['-h'])
        exit()

    flagstats = readin_flagstats(args.flagstats)
    calculate_exp_depth(flagstats, args.depths, args.outfile, args.force_overwrite)